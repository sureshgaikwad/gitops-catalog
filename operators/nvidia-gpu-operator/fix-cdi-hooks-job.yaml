apiVersion: batch/v1
kind: Job
metadata:
  name: fix-cdi-hooks
  namespace: nvidia-gpu-operator
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation,HookSucceeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: fix-cdi-hooks
    spec:
      serviceAccountName: fix-cdi-hooks
      restartPolicy: OnFailure
      containers:
      - name: fix-cdi
        image: quay.io/openshift/origin-cli:latest
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "============================================================"
          echo "Fixing CDI Hook Issues on All GPU Nodes"
          echo "============================================================"
          echo ""
          
          # Wait for GPU operator to be ready (check actual pods, not just daemonset status)
          echo "1. Waiting for NVIDIA GPU Operator to be ready..."
          MAX_WAIT=300
          ELAPSED=0
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            # Check if namespace exists first
            if ! oc get namespace nvidia-gpu-operator &>/dev/null; then
              echo "   Namespace nvidia-gpu-operator does not exist yet... ($ELAPSED/$MAX_WAIT seconds)"
              sleep 10
              ELAPSED=$((ELAPSED + 10))
              continue
            fi
            
            # Check for actual running pods - try multiple label selectors and name patterns
            READY_FOUND=false
            
            # Try different label selectors
            for LABEL_SELECTOR in "app=nvidia-container-toolkit" "app.kubernetes.io/name=nvidia-container-toolkit" "app.kubernetes.io/component=container-toolkit" ""; do
              if [ -z "$LABEL_SELECTOR" ]; then
                # Last attempt: check all pods in namespace
                PODS=$(oc get pods -n nvidia-gpu-operator --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
              else
                PODS=$(oc get pods -n nvidia-gpu-operator -l "$LABEL_SELECTOR" --field-selector=status.phase=Running -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
              fi
              
              if [ -n "$PODS" ]; then
                # Check if any pod contains "toolkit" or "container" in name, or just check if any pod is ready
                for POD_NAME in $PODS; do
                  # Check if this pod is ready
                  POD_READY=$(oc get pod -n nvidia-gpu-operator $POD_NAME -o jsonpath='{.status.containerStatuses[0].ready}' 2>/dev/null || echo "false")
                  if [ "$POD_READY" = "true" ]; then
                    echo "   ✓ GPU Operator pod $POD_NAME is running and ready"
                    READY_FOUND=true
                    break 2
                  fi
                done
                
                # If we found pods but none are ready, check if any pod name contains toolkit/container
                for POD_NAME in $PODS; do
                  if echo "$POD_NAME" | grep -qiE "(toolkit|container|nvidia)"; then
                    echo "   GPU Operator pod $POD_NAME is running (assuming ready)"
                    READY_FOUND=true
                    break 2
                  fi
                done
              fi
            done
            
            # If no pods found with labels, check for CDI spec files directly (most reliable)
            if [ "$READY_FOUND" = "false" ]; then
              GPU_NODES_CHECK=$(oc get nodes -l nvidia.com/gpu.present=true -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
              if [ -n "$GPU_NODES_CHECK" ]; then
                # Check if CDI spec exists on at least one node
                for CHECK_NODE in $GPU_NODES_CHECK; do
                  if oc debug node/$CHECK_NODE -- chroot /host test -f /var/run/cdi/k8s.device-plugin.nvidia.com-gpu.json 2>/dev/null; then
                    echo "   ✓ CDI spec file found on node $CHECK_NODE, GPU operator is ready"
                    READY_FOUND=true
                    break
                  fi
                done
              fi
            fi
            
            if [ "$READY_FOUND" = "true" ]; then
              break
            fi
            
            echo "   Waiting for GPU Operator... ($ELAPSED/$MAX_WAIT seconds)"
            sleep 10
            ELAPSED=$((ELAPSED + 10))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "   ⚠ WARNING: GPU Operator readiness check timed out, but continuing anyway..."
            echo "   (This is OK if GPU operator is actually running)"
          fi
          
          # Wait a bit more for CDI specs to be generated on all nodes
          echo ""
          echo "2. Waiting for CDI specs to be generated..."
          sleep 15
          
          # Get all nodes with GPUs
          echo ""
          echo "3. Finding nodes with GPUs..."
          GPU_NODES=$(oc get nodes -l nvidia.com/gpu.present=true -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
          
          if [ -z "$GPU_NODES" ]; then
            echo "   Checking all nodes for GPU labels..."
            GPU_NODES=$(oc get nodes -o jsonpath='{.items[*].metadata.name}')
          fi
          
          if [ -z "$GPU_NODES" ]; then
            echo "   ERROR: No nodes found!"
            exit 1
          fi
          
          echo "   Found nodes: $GPU_NODES"
          echo ""
          
          # Fix CDI spec on each node using oc debug
          FIXED_COUNT=0
          FAILED_COUNT=0
          
          for NODE in $GPU_NODES; do
            echo "4. Fixing CDI spec on node: $NODE"
            
            # Pre-encoded Python script (base64) to avoid YAML parsing issues with heredocs
            PYTHON_SCRIPT_B64="aW1wb3J0IGpzb24KaW1wb3J0IHN5cwogCmNkaV9maWxlID0gIi92YXIvcnVuL2NkaS9rOHMuZGV2aWNlLXBsdWdpbi5udmlkaWEuY29tL2dwdS5qc29uIgogCnRyeToKICAgIHdpdGggb3BlbihjZGlfZmlsZSwgInIiKSBhcyBmOgogICAgICAgIGRhdGEgPSBqc29uLmxvYWQoZikKICAgIAogICAgIyBSZW1vdmUgYWxsIGhvb2tzCiAgICBpZiAiY29udGFpbmVyRWRpdHMiIGluIGRhdGE6CiAgICAgICAgaWYgImhvb2tzIiBpbiBkYXRhWyJjb250YWluZXJFZGl0cyJdOgogICAgICAgICAgICBwcmludChmIiAgICBSZW1vdmluZyB7bGVuKGRhdGFbJ2NvbnRhaW5lckVkaXRzJ11bJ2hvb2tzJ10pfSBob29rcy4uLiIpCiAgICAgICAgICAgIGRhdGFbImNvbnRhaW5lckVkaXRzIl1bImhvb2tzIl0gPSBbXQogICAgICAgIAogICAgICAgICMgU2V0IGVudmlyb25tZW50IHZhcmlhYmxlcwogICAgICAgIGRhdGFbImNvbnRhaW5lckVkaXRzIl1bImVudiJdID0gWwogICAgICAgICAgICAiTlZJRElBX0NUS19MSUJDVURBX0RJUj0vdXNyL2xpYjY0IiwKICAgICAgICAgICAgIk5WSURJQV9WSVNJQkxFX0RFVklDRVM9YWxsIgogICAgICAgIF0KICAgICAgICBwcmludCgiICAg4oiSIFNldCBOVklESUFfVklTSUJMRV9ERVZJQ0VTPWFsbCIpCiAgICAKICAgICMgV3JpdGUgdGhlIGZpeGVkIHNwZWMKICAgIHdpdGggb3BlbihjZGlfZmlsZSwgInciKSBhcyBmOgogICAgICAgIGpzb24uZHVtcChkYXRhLCBmLCBpbmRlbnQ9MikKICAgIAogICAgcHJpbnQoIiAgIOKIkiBDREkgc3BlYyBmaXhlZCBzdWNjZXNzZnVsbHkiKQogICAgCmV4Y2VwdCBGaWxlTm90Rm91bmRFcnJvcjoKICAgIHByaW50KCIgICDwn5KPIENESSBzcGVjIGZpbGUgbm90IGZvdW5kIikKICAgIHN5cy5leGl0KDApCmV4Y2VwdCBqc29uLkpTT05EZWNvZGVFcnJvciBhcyBlOgogICAgcHJpbnQoZiIgICDiiJMgRXJyb3IgcGFyc2luZyBKU09OOiB7ZX0iKQogICAgc3lzLmV4aXQoMSkKZXhjZXB0IEV4Y2VwdGlvbiBhcyBlOgogICAgcHJpbnQoZiIgICDiiJMgRXJyb3I6IHtlfSIpCiAgICBzeXMuZXhpdCgxKQ=="
            
            # Try to fix the CDI spec on this node
            if oc debug node/$NODE -- chroot /host /bin/bash -c "
              set -e
              CDI_SPEC_FILE=\"/var/run/cdi/k8s.device-plugin.nvidia.com-gpu.json\"
              
              # Check if file exists
              if [ ! -f \"\$CDI_SPEC_FILE\" ]; then
                echo '   ⚠ CDI spec file not found, skipping...'
                exit 0
              fi
              
              # Check if Python is available
              if ! command -v python3 &> /dev/null; then
                echo '   ✗ ERROR: python3 not found on node'
                exit 1
              fi
              
              # Check if base64 is available
              if ! command -v base64 &> /dev/null; then
                echo '   ✗ ERROR: base64 not found on node'
                exit 1
              fi
              
              # Backup the original file
              if [ ! -f \"\${CDI_SPEC_FILE}.backup\" ]; then
                cp \"\$CDI_SPEC_FILE\" \"\${CDI_SPEC_FILE}.backup\" || {
                  echo '   ✗ ERROR: Failed to create backup'
                  exit 1
                }
                echo '   ✓ Created backup'
              else
                echo '   ✓ Backup already exists'
              fi
              
              # Decode and run Python script
              echo '$PYTHON_SCRIPT_B64' | base64 -d | python3 || {
                echo '   ✗ ERROR: Failed to decode or execute Python script'
                exit 1
              }
            " 2>&1; then
              FIXED_COUNT=$((FIXED_COUNT + 1))
              echo "   ✓ Successfully fixed CDI spec on node $NODE"
            else
              FAILED_COUNT=$((FAILED_COUNT + 1))
              echo "   ✗ Failed to fix CDI spec on node $NODE (check logs above for details)"
            fi
            
            echo ""
          done
          
          echo "============================================================"
          echo "CDI Hook Fix Complete"
          echo "============================================================"
          echo "Nodes fixed: $FIXED_COUNT"
          echo "Nodes failed: $FAILED_COUNT"
          echo ""
          
          if [ $FAILED_COUNT -gt 0 ]; then
            echo "WARNING: Some nodes failed to fix."
            exit 1
          fi
          
          echo "✓ All nodes fixed successfully!"
          echo ""
          echo "Note: The CDI spec files may be regenerated by the container-toolkit"
          echo "daemonset. If the issue persists, restart the daemonset."
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi

