apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: mistral-small-3-1-24b-instruct-2503-quantized-w4a16 # OPTIONAL CHANGE
    serving.kserve.io/deploymentMode: RawDeployment
    argocd.argoproj.io/sync-wave: "1"
#    security.opendatahub.io/enable-auth: "true"  # Critical for token auth
#    serving.knative.openshift.io/enablePassthrough: "true"  # Critical for auth
#    sidecar.istio.io/inject: "true"  # For service mesh
#    sidecar.istio.io/rewriteAppHTTPProbers: "true"  # For health checks
  name: mistral-small-24b-instruct-quantized       # specify model name. This value will be used to invoke the model in the payload
  namespace: models
  labels:
    opendatahub.io/dashboard: 'true'
    modelregistry.opendatahub.io/registered: 'true'
spec:
  predictor:
#    automountServiceAccountToken: false 
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '20'			# this is model specific
          memory: 80Gi		# this is model specific
          nvidia.com/gpu: '1'	# this is accelerator specific
        requests:			# same comment for this block
          cpu: '20'
          memory: 80Gi
          nvidia.com/gpu: '1'
      runtime: vllm-cuda-runtime	# must match the ServingRuntime name above
      storageUri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-3-1-24b-instruct-2503-quantized-w4a16:1.5
      args:
        - "--dtype=half"   # T4 (compute 7.5) does not support bfloat16; use float16
        - "--max-model-len=30000"
        - "--gpu-memory-utilization=0.95"
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
